import pandas as pd
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from scipy import sparse
from sklearn.ensemble import RandomForestRegressor

df = pd.read_csv("data/raw/raw_data.csv")

# Data Structure
print("Data Structure")
print("---------------")
print(f"Dimensions: {df.shape}")
print(f"Data Types:\n{df.dtypes}")
print(f"Missing Values:\n{df.isnull().sum()}")

"""
This data contains 1339 rows and 44 columns. 

"Number.of.Bags", "Category.One.Defects", and "Category.Two.Defects" are stored as integer data type.
"Aroma", "Flavor", "Aftertaste", "Acidity", "Body", "Balance", "Uniformity", "Clean.Cup", "Sweetness", "Cupper.Points", "Total.Cup.Points", "Moisture",
"Quakers", "altitude_low_meters", "altitude_high_meters", and "altitude_mean_meters" are stored as float data type. "Species", "Owner",
"Country.of.Origin", "Farm.Name", "Lot.Number", "Mill", "ICO.Number", "Company", "Altitude", "Region", "Producer", "Bag.Weight", "In.Country.Partner",
"Harvest.Year", "Grading.Date", "Owner.1", "Variety", "Processing.Method", "Color", "Expiration", "Certification.Body", "Certification.Address",
"Certification.Contact", and "unit_of_measurement" are stored as strings.
"""

# Owner is missing 7 entries. Owner should have impact on coffee quality, represented by Total.Cup.Points. Therefore, rows with empty entries can be dropped.
df.dropna(subset=["Owner"], inplace=True)

# Country.of.Origin (missing 1 value) will affect coffee quality, so instead of dropping this column, we will drop the row that is missing an entry.
df.dropna(subset=["Country.of.Origin"], inplace=True)

# Farm.Name is missing 359 entries. This has no impact of coffee quality, so drop this column.
df.drop(columns=["Farm.Name"], inplace=True)

# Lot.Number is missing 1063 entries. There are too many missing entries, so drop this column.
df.drop(columns=["Lot.Number"], inplace=True)

# Owner.1 is only missing 7 entries, drop rows where na.
df.dropna(subset=["Owner.1"], inplace=True)

# Quakers is only missing 1 entry, so throw out row where missing value
df.dropna(subset=["Quakers"], inplace=True)

"""
Allow the rest of this data to be preprocessed by the imputer in the data pipeline.
Columns that should not be filled by mean or most common removed here.
"""

# Data Structure After Manual Preprocessing
print("Data Structure After Manual Preprocessing")
print("---------------")
print(f"Dimensions: {df.shape}")
print(f"Missing Values:\n{df.isnull().sum()}")

# Pipeline preprocesssing 

numeric_cols = ["Number.of.Bags", "Category.One.Defects", "Category.Two.Defects", "Aroma", "Flavor", "Aftertaste", "Acidity", "Body", "Balance", "Uniformity", "Clean.Cup", "Sweetness", "Cupper.Points", "Moisture",
"Quakers", "altitude_low_meters", "altitude_high_meters", "altitude_mean_meters"]
categorical_cols = ["Species", "Owner", "Country.of.Origin", "Mill", "ICO.Number", "Company", "Altitude", "Region", "Producer", "Bag.Weight", "In.Country.Partner",
"Harvest.Year", "Grading.Date", "Owner.1", "Variety", "Processing.Method", "Color", "Expiration", "Certification.Body", "Certification.Address",
"Certification.Contact", "unit_of_measurement"]

"""
The below block of code was derived from AIPI503 - Ed Lessons Day 4 Challenge
This course was taught by Dr. Daniel E. Davis, Ph.D.
"""
X = df.drop(columns=["Total.Cup.Points"])
y = df["Total.Cup.Points"]


train_df, test_df, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

def preprocess(train_df, test_df, numeric_cols, categorical_cols):
    imputer_num = SimpleImputer(strategy="median")
    train_num_imp = imputer_num.fit_transform(train_df[numeric_cols])
    test_num_imp = imputer_num.transform(test_df[numeric_cols])

    scaler_num = StandardScaler()
    train_num_scaled = scaler_num.fit_transform(train_num_imp)
    test_num_scaled = scaler_num.transform(test_num_imp)

    imputer_cat = SimpleImputer(strategy="most_frequent")
    train_cat_imp = imputer_cat.fit_transform(train_df[categorical_cols])
    test_cat_imp = imputer_cat.transform(test_df[categorical_cols])

    ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
    train_cat_enc = ohe.fit_transform(train_cat_imp)
    test_cat_enc = ohe.transform(test_cat_imp)

    ohe_feature_names = ohe.get_feature_names_out(categorical_cols)

    feature_names = list(numeric_cols) + list(ohe_feature_names)
    X_train = np.hstack([train_num_scaled, train_cat_enc])
    X_test  = np.hstack([test_num_scaled,  test_cat_enc])
    # The above code snipet was generated by chatGPT 5.1 at 12:07p on 11/20/25.
    
    X_train_df = pd.DataFrame(X_train, columns=feature_names, index=train_df.index)
    X_test_df  = pd.DataFrame(X_test,  columns=feature_names, index=test_df.index)
    # The above code snipet was generated by chatGPT 5.1 at 12:27p on 11/20/25.

    X_train_df.to_csv("data/cleaned/X_train.csv", index=False)
    X_test_df.to_csv("data/cleaned/X_test.csv", index=False)
    y_train.to_csv("data/cleaned/y_train.csv", index=False)
    y_test.to_csv("data/cleaned/y_test.csv", index=False)

    return X_train, X_test, feature_names, imputer_num, scaler_num, imputer_cat, ohe

"""
End cited block
"""

preprocess(train_df, test_df, numeric_cols, categorical_cols)