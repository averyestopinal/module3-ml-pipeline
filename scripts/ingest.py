"""
ingest.py: 
- Reads raw dataset from Kaggle CSV.
- Drops unwanted/missing columns.
- Saves data/preprocessed/preprocessed_data.csv.
- Saves data to Supabase
"""
import pandas as pd
import os
import yaml
from supabase import create_client, Client

"""
Citation:
OpenAI. (2025). ChatGPT (Version 5.1) [Large language model]. https://chat.openai.com  
Conversation with ChatGPT on November 20, 2025, used to generate preprocessing code snippets.
"""

with open("config.yaml", "r") as f:
    config = yaml.safe_load(f)

raw_path = config["data"]["local_path"]
preprocessed_path = config["data"]["preprocessed_path"]
target_col = config["data"]["target"]

test_size = config["train"]["test_size"]
random_state = config["train"]["random_state"]

"""
End cited block
"""
df = pd.read_csv(raw_path)

# Data Structure
print("Data Structure")
print("---------------")
print(f"Dimensions: {df.shape}")
print(f"Data Types:\n{df.dtypes}")
print(f"Missing Values:\n{df.isnull().sum()}")

"""
This data contains 1339 rows and 44 columns. 

"Number.of.Bags", "Category.One.Defects", and "Category.Two.Defects" are stored as integer data type.
"Aroma", "Flavor", "Aftertaste", "Acidity", "Body", "Balance", "Uniformity", "Clean.Cup", "Sweetness", "Cupper.Points", "Total.Cup.Points", "Moisture",
"Quakers", "altitude_low_meters", "altitude_high_meters", and "altitude_mean_meters" are stored as float data type. "Species", "Owner",
"Country.of.Origin", "Farm.Name", "Lot.Number", "Mill", "ICO.Number", "Company", "Altitude", "Region", "Producer", "Bag.Weight", "In.Country.Partner",
"Harvest.Year", "Grading.Date", "Owner.1", "Variety", "Processing.Method", "Color", "Expiration", "Certification.Body", "Certification.Address",
"Certification.Contact", and "unit_of_measurement" are stored as strings.
"""

# Owner is missing 7 entries. Owner should have impact on coffee quality, represented by Total.Cup.Points. Therefore, rows with empty entries can be dropped.
df.dropna(subset=["Owner"], inplace=True)

# Country.of.Origin (missing 1 value) will affect coffee quality, so instead of dropping this column, we will drop the row that is missing an entry.
df.dropna(subset=["Country.of.Origin"], inplace=True)

# Farm.Name is missing 359 entries. This has no impact of coffee quality, so drop this column.
df.drop(columns=["Farm.Name"], inplace=True)

# Lot.Number is missing 1063 entries. There are too many missing entries, so drop this column.
df.drop(columns=["Lot.Number"], inplace=True)

# Owner.1 is only missing 7 entries, drop rows where na.
df.dropna(subset=["Owner.1"], inplace=True)

# Quakers is only missing 1 entry, so throw out row where missing value.
df.dropna(subset=["Quakers"], inplace=True)

"""
Allow the rest of this data to be preprocessed by the imputer in the data pipeline.
Columns that should not be filled by mean or most common removed here.
"""

# Data Structure After Manual Preprocessing
print("Data Structure After Manual Preprocessing")
print("---------------")
print(f"Dimensions: {df.shape}")
print(f"Missing Values:\n{df.isnull().sum()}")

os.makedirs(os.path.dirname(preprocessed_path), exist_ok=True)
# The above code snipet was generated by chatGPT 5.1 at 11:05p on 11/20/25.


# Cited Block (Saving to Supabase): #
"""
Cited Block:
OpenAI. (2025, November 23). ChatGPT (Version 5.1). Supabase Storage example 
using the Python client. https://chat.openai.com/
"""

supabase_url = config["supabase"]["url"]
supabase_key = config["supabase"]["key"]
bucket_name = config["supabase"]["bucket"]

supabase: Client = create_client(supabase_url, supabase_key)

destination_path = "preprocessed/preprocessed_data.csv"

# Delete existing file to allow overwrite
try:
    supabase.storage.from_(bucket_name).remove([destination_path])
except Exception:
    pass  # OK if file does not exist

# Upload file (correct Supabase Python syntax)
with open(preprocessed_path, "rb") as f:
    file_bytes = f.read()

supabase.storage.from_(bucket_name).upload(
    destination_path, 
    file_bytes
)

print(f"Uploaded cleaned data to Supabase bucket '{bucket_name}' at '{destination_path}'")
# ---------------------------------END CITED BLOCK---------------------