import os
import yaml
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder

"""
Cited block from chatGPT 5.1 at 10:10p on 11/20/25. 

Citation:
OpenAI. (2025). ChatGPT (Version 5.1) [Large language model]. https://chat.openai.com  
Conversation with ChatGPT on November 20, 2025, used to generate preprocessing code snippets.
"""

with open("config.yaml", "r") as f:
    config = yaml.safe_load(f)

raw_path = config["data"]["local_path"]
preprocessed_path = config["data"]["preprocessed_path"]
target_col = config["data"]["target"]

test_size = config["train"]["test_size"]
random_state = config["train"]["random_state"]

"""
End cited block
"""

df = pd.read_csv(preprocessed_path)

numeric_cols = ["Number.of.Bags", "Category.One.Defects", "Category.Two.Defects", "Aroma", "Flavor", "Aftertaste", "Acidity", "Body", "Balance", "Uniformity", "Clean.Cup", "Sweetness", "Cupper.Points", "Moisture",
"Quakers", "altitude_low_meters", "altitude_high_meters", "altitude_mean_meters"]
categorical_cols = ["Species", "Owner", "Country.of.Origin", "Mill", "ICO.Number", "Company", "Altitude", "Region", "Producer", "Bag.Weight", "In.Country.Partner",
"Harvest.Year", "Grading.Date", "Owner.1", "Variety", "Processing.Method", "Color", "Expiration", "Certification.Body", "Certification.Address",
"Certification.Contact", "unit_of_measurement"]

"""
The below block of code was derived from AIPI503 - Ed Lessons Day 4 Challenge
This course was taught by Dr. Daniel E. Davis, Ph.D.
"""

if target_col not in df.columns:
    raise ValueError(f"ERROR: Required target column '{"Total.Cup.Points"}' is missing from the dataset.")

X = df.drop(columns=target_col)
y = df[target_col]

# Splitting data. 20% test, 80% train
train_df, test_df, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

def preprocess(train_df, test_df, numeric_cols, categorical_cols):
    imputer_num = SimpleImputer(strategy="median")
    train_num_imp = imputer_num.fit_transform(train_df[numeric_cols])
    test_num_imp = imputer_num.transform(test_df[numeric_cols])

    scaler_num = StandardScaler()
    train_num_scaled = scaler_num.fit_transform(train_num_imp)
    test_num_scaled = scaler_num.transform(test_num_imp)

    imputer_cat = SimpleImputer(strategy="most_frequent")
    train_cat_imp = imputer_cat.fit_transform(train_df[categorical_cols])
    test_cat_imp = imputer_cat.transform(test_df[categorical_cols])

    ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
    train_cat_enc = ohe.fit_transform(train_cat_imp)
    test_cat_enc = ohe.transform(test_cat_imp)

    ohe_feature_names = ohe.get_feature_names_out(categorical_cols)

    feature_names = list(numeric_cols) + list(ohe_feature_names)
    X_train = np.hstack([train_num_scaled, train_cat_enc])
    X_test  = np.hstack([test_num_scaled,  test_cat_enc])
    # The above code snipet was generated by chatGPT 5.1 at 12:07p on 11/20/25.
    
    X_train_df = pd.DataFrame(X_train, columns=feature_names, index=train_df.index)
    X_test_df  = pd.DataFrame(X_test,  columns=feature_names, index=test_df.index)
    # The above code snipet was generated by chatGPT 5.1 at 12:27p on 11/20/25.

    os.makedirs("data/cleaned", exist_ok=True)
    # The above code snipet was generated by chatGPT 5.1 at 10:00p on 11/20/25.

    X_train_df.to_csv(config["paths"]["X_train"], index=False)
    X_test_df.to_csv(config["paths"]["X_test"], index=False)
    y_train.to_csv(config["paths"]["y_train"], index=False)
    y_test.to_csv(config["paths"]["y_test"], index=False)
    # File locations generated by chatGPT 5.1 at 10:15p on 11/20/25.

    return X_train, X_test, feature_names, imputer_num, scaler_num, imputer_cat, ohe

"""
End cited block
"""

preprocess(train_df, test_df, numeric_cols, categorical_cols)